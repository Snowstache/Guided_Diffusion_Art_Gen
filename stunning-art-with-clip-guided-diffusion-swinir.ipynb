{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":3170972,"sourceType":"datasetVersion","datasetId":1927476}],"dockerImageVersionId":30163,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<br>\n<p style='text-align: left;'><span style=\"color: #353535; font-family: Tahoma; font-size: 2.6em; font-weight: 600;\">Generate Stunning Artworks with CLIP Guided Diffusion + SwinIR Super Resolution</span></p>\n\n<p style='text-align: left;'><span style=\"color: #767676; font-family: Arial; font-size: 1.4em; font-weight: 400;\">Create beautiful artworks by fine-tuning diffusion models on custom datasets, and performing CLIP guided text-conditional sampling, followed by SWIN-transformer based super-resolution</span></p>\n\n<br>\n<br>\n\n![](https://i.ibb.co/H2sHF0T/cover1-02.jpg)\n\n<br>\n<br>\n\n<span style=\"background-color: #EFFFCD;\">üìå Throughout this notebook, we will be using a codebase I have put together:</span><br>\n<p style='text-align: left;'>\n    <span>\n    &emsp;<a href=\"https://github.com/sreevishnu-damodaran/clip-diffusion-art\"><img alt=\"github.com/sreevishnu-damodaran/clip-diffusion-art\" src=\"https://img.shields.io/badge/sreevishnu--damodaran%2Fclip--diffusion--art-2B2E3A?style=flat&logo=github&logoColor=white\" width=\"300\">\n        </a>\n    </span>\n</p>\n<br>\n\n<span style=\"background-color: #EFFFCD;\">üìå Dataset with public domain artworks created for this project:</span><br>\n<span>\n&emsp;<a href=\"https://www.kaggle.com/sreevishnudamodaran/artworks-in-public-domain\">kaggle.com/sreevishnudamodaran/artworks-in-public-domain\n</a>\n</span><br><br>\n\n<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Overview</span>\n\n***Topics covered in this notebook:***\n\n&emsp;‚úîÔ∏è&ensp;Fine-tune Diffusion Models on Custom Datasets<br>\n&emsp;‚úîÔ∏è&ensp;Zero-shot CLIP Guided Diffusion Sampling<br>\n&emsp;‚úîÔ∏è&ensp;Denoising Diffusion Implicit Model Sampling<br>\n&emsp;‚úîÔ∏è&ensp;SwinIR Super-resolution Using Shifted Window Transformer<br>\n&emsp;‚úîÔ∏è&ensp;A Brief Overview of Diffusion models, CLIP & SwinIR<br>\n&emsp;‚úîÔ∏è&ensp;Experiment Tracking & Interactive Visualizations with Weights & Biases<br>\n\n<br>\n\nLet's explore the creative capabilities of deep generative models and take a deep dive into how we can make use of these models, in combination with generalized vision-language models, to create beautiful artworks of various styles from natural language text prompts.\n\nWe will look at how to fine-tune diffusion probabilistic models on a custom dataset created from artworks in the public domain. During the sampling process to generate images, we will use a <span style=\"color: #DC143C;\">vision-language CLIP model to steer or guide</span> this fine-tuned model with <span style=\"color: #DC143C;\">natural language prompts</span> without any extra training or supervision. Afterwards, the generated images will be enlarged to a larger size by using a Swin transformer-based super-resolution model, which turns the low resolution generated output into a high resolution image by generating finer realistic details, and enhancing visual quality. We will also briefly cover the concepts behind the inner workings of each of these models, and more details on integrating them in a bit.\n\n***Here is a general block diagram showing the various components.***\n\n<br>\n\n<div style=\"text-align:center\">\n    <img src=\"https://i.ibb.co/3sTH15m/diagram-final3.jpg\" alt=\"drawing\" width=\"700\" style=\"padding: 10px;\"/>\n    <p style='text-align: center;'>\n        <span style=\"color: #353535; font-size:0.9em; font-family: Verdana; font-weight: 300; letter-spacing: 0px;\">General block diagram</span>\n    </p>\n</div>\n<br>\n\n\n***Here are some examples of the artwork generation process from text prompts, using the final fine-tuned model with CLIP guidance:***\n\n<div style=\"text-align:center\">\n    <img src=\"https://i.ibb.co/DpTYvK3/job18-1.gif\" alt=\"drawing\" width=\"250\" style=\"padding: 20px;\"/>\n    <img src=\"https://i.ibb.co/8gsR0w1/2.gif\" alt=\"drawing\" width=\"250\" style=\"padding: 20px;\"/>\n    <p style='text-align: center;'>\n        <span style=\"color: #353535; font-size:0.9em; font-family: Verdana; font-weight: 300; letter-spacing: 0px;\">Generated samples for prompts <span style=\"color: #DC143C;\">\"vibrant watercolor painting of a flower, artstation HQ\"</span> and <span style=\"color: #DC143C;\">\"artstation HQ, photorealistic depiction of an alien city\".</span></span>\n    </p>\n</div>\n<br>\n<span style=\"font-size:1.2em;\">\n    <a href=\"https://wandb.ai/sreevishnu-damodaran/clip_diffusion_art/reports/Results-CLIP-Guided-Diffusion-SwinIR--VmlldzoxNjUxNTMz\">üîé Visit this report for more generated artworks ‚ûî <br><br>\n    </a>\n    <div style=\"text-align:center\">\n        <a href=\"https://wandb.ai/sreevishnu-damodaran/clip_diffusion_art/reports/Results-CLIP-Guided-Diffusion-SwinIR--VmlldzoxNjUxNTMz\">\n            <img alt=\"Report gif\" src=\"https://i.ibb.co/GHXJhyX/report-opti.gif\" width=\"700\">\n        </a>\n    </div>\n</span>\n<br>\n<br>\n\n<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Diffusion Models</span>\n\nOver the years, <span style=\"color: #DC143C;\">deep generative models</span> have evolved to model complex high-dimensional probability distributions across a range of perceptive and predictive tasks. These were accomplished by well-formulated neural network architectures and parametrization techniques. For sometime, Generative Adversarial Networks (GANs), Variational Auto-Encoders (VAEs) and Flow-based models were the front runners of this area. In spite of the vast number of milestones that are getting accomplished with these models, they suffer from a range of shortcomings in terms of training stability, lack of diversity, and high sensitivity to changes in hyper-parameters.\n\nDiffusion Probabilistic models, a new family of models were introduced by [Sohl-Dickstein et al.](http://proceedings.mlr.press/v37/sohl-dickstein15.html) in 2015 to try to overcome these weaknesses, or rather to traverse other ways to solve generative tasks. They were inspired by non-equilibrium thermodynamics. Several papers and improvements later, they have now achieved competitive log likelihoods and state-of-the-art results across a wide variety of tasks,  maintaining better characteristics compared to its counterparts in terms of training stability and improved diversity in image synthesis.\n\n<br>\n<div style=\"text-align:center\">\n    <img src=\"https://i.ibb.co/4JDnP9G/diff.jpg\" alt=\"drawing\" width=\"600\" style=\"padding: 20px;\"/>\n    <p style='text-align: center;'>\n        <span style=\"color: #353535; font-size:0.9em; font-family: Verdana; font-weight: 300; letter-spacing: 0px;\">Forward and reverse diffusion process.</span>\n    </p>\n</div>\n<br>\n\nThe key idea behind diffusion models is the use of a parameterized Markov chain, which is trained to produce samples from a data distribution by reversing a gradual, multi-step noising process starting from a pure noise $x_T$,and denoising at every step to produce less noisy samples $x_{T ‚àí1}, x_{T ‚àí2},$ ‚Ä¶ reaching the final synthesized sample $x_0$. Contrary to initial work on these models, it was later found that parameterizing this model as a function of the noise with respect to x_t and t, which predicts the noise component of a noisy sample x_t is better than predicting the noisy image x_t itself (Ho et al.). To train these models, each sample in a mini-batch is produced by randomly drawing a data sample x_0, a timestep t, and a noise epsilon, which together are used to produce a noisy sample x_t. The training objective is then $||\\epsilon_Œ∏(x_t, t) - \\epsilon||^2$ i.e. a simple mean-squared error loss between the true noise and the predicted noise. The approximation of the reverse predicted noise is done by a neural network, since these predictions depend on the entire data distribution, which is unknown. So, the latent information of the training data distribution is stored in the neural network part of the model.\n\nWe will be using diffusion model architectures and training procedures from the papers Improved Denoising Diffusion Probabilistic Models and Diffusion Models Beat GANs by Dhariwal and Nichol, 2021 (OpenAI), where the authors have improved the log-likelihood to maximize the learning of all modes of the data distribution, and other generative metrics like FID (Fr√©chet Inception Distance) and IS (Inception Score), to enhance the generated image fidelity. The model we will use has a neural network architecture based on the backbone of PixelCNN++, which is a U-Net based on a Wide ResNet with group normalization instead of weight normalization, to make the implementation simpler. These models have two convolutional residual blocks per resolution level, and use multi-head self-attention blocks at the 16√ó16 resolution and 8x8 resolution between the convolutional blocks. Diffusion time $t$ is specified by adding the transformer sinusoidal position embedding into each residual block.\n\nThere are several other intricacies to understanding diffusion models with many improvements in recent literature, which all would be hard to summarize in a short article. For a better theoretical understanding and details on the implementation, I recommend going through the papers on diffusion models. At the time of writing this article, the total count of papers on diffusion models is not as overwhelming as the number of GANs papers.\n\n\n<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">A Faster Way of Sampling with DDIMs</span>\n\n> ***DDPMs inherently suffer from the need to sample hundreds-to-thousands of steps to generate a high fidelity sample, making them prohibitively expensive and impractical in real-world applications, where the data tends to be high-dimensional.***\n\nA solution to get around this problem was to shift to the use of non-Markovian diffusion processes instead of Markovian diffusion processes (used in DDPMs) during sampling. This new class of models were called DDIMs (Denoising Diffusion Implicit Models), which follow the same training procedure as that of DDPMs to train for an arbitrary number of forward steps. The reverse process is performed with new generative processes, which enable sampling faster in only a subset of those forward steps during generation. The authors showed that DDIMs can produce high quality samples <span style=\"color: #DC143C;\">10x to 50x</span> faster compared to DDPMs.\n\n<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Steering Gradients with CLIP</span>\n\nCLIP (Contrastive Language‚ÄìImage Pre-training) has set a benchmark in the areas of <span style=\"color: #DC143C;\">zero-shot transfer, natural language supervision, and multi-modal learning</span>, by means of training on a wide variety of images and language supervision. These models are not trained directly to optimize on the benchmarks of singular tasks, making them far less short-sighted on the visual and language concepts learned. This led to better performance compared to several supervised ImageNet-trained models, even surpassing the original ResNet50 without being trained explicitly on any of the 1.28M labeled samples. CLIP has been used in a wide variety of tasks since it was introduced in January, 2021.\n\n<br>\n<div style=\"text-align:center\">\n    <img src=\"https://i.ibb.co/Jxxg4CK/clip.jpg\" alt=\"drawing\" width=\"650\"/>\n    <p style='text-align: center;'>\n        <span style=\"color: #353535; font-size:0.9em; font-family: Verdana; font-weight: 300; letter-spacing: 0px;\">Comparison of CLIP with other models.</span>\n    </p>\n</div>\n<br>\n\nThe authors used a large dataset created with around 400 million image-text pairs for training. In every iteration, a batch of $N$ pairs of text and images are forwarded through an image and text encoder, which trains jointly to maximize the cosine similarity of the text and image embeddings of the $N$ real pairs (in the diagonal elements of the multi-modal embedding space represented in the figure below), while minimizing the similarity scores of the other $N^2-N$ elements (present at the non-diagonal positions) in the embedding space, to form a contrastive training objective. A symmetric cross-entropy loss is used to optimize the model on these similarity scores.\n\n<br>\n<div style=\"text-align:center\">\n    <img src=\"https://i.ibb.co/jZyf44F/clip3.jpg\" alt=\"drawing\" width=\"550\"/>\n    <p style='text-align: center;'>\n        <span style=\"color: #353535; font-size:0.9em; font-family: Verdana; font-weight: 300; letter-spacing: 0px;\">CLIP training process.</span>\n    </p>\n</div>\n<br>\n\nWe will use CLIP to steer the image sampling denoising process of diffusion models, to produce samples matching the text prompt provided as a condition. This technique has been used in works like DALL-E and GLIDE, and also to guide other generative models like VQGAN, StyleGAN2 and Siren (Sinusoidal Representation Networks) to name a few. This guidance procedure is done by first encoding the intermediate output image of the diffusion model during the iterative sampling process with the CLIP image encoder head, while the text prompts are converted to embeddings by using the text encoder head. Then, the resultant output image and text embeddings are used to compute a perceptual loss, which measures the similarity between the two embeddings. The <span style=\"color: #DC143C;\">gradients with respect to this loss and the intermediate denoised image are used for conditioning</span>, or guiding the diffusion model during the sampling process to produce the next intermediate denoised image. This process is repeated until the total sampling steps are complete. We also use losses to control spatial smoothing like total variation and range losses, as well as image augmentations, to improve the quality. In addition to this, multiple cutouts of images are also taken in batches to minimize the loss objective, leading to improvements in the synthesis quality, and optimized memory usage when sampling from smaller GPUs.\n\n<br>\n<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Upscaling Generated Images with Super-resolution</span>\n\nLarge deep generative models need to be trained on large GPU clusters for days or even weeks. On single and smaller GPUs, we are limited to being able to train 256x256 diffusion models, which can only output images with less visual detail. So, we will work around this by training a smaller 256x256 output model, and upscaling its predictions 3x times to obtain the final images of a larger size of 1024x1024. Conventional upscaling to enlarge images by using interpolation techniques such as bilinear or lanczos, results in degradation of image quality and blurring artifacts, as no new visual detail gets added. An easy remedy to this problem is to use a super-resolution model trained to recover the finer details by a generative process. This produces enlarged images with <span style=\"color: #DC143C;\">high perceptual quality and peak signal-to-noise ratio (PSNR)</span>.\n\n<br>\n<div style=\"text-align:center\">\n    <img src=\"https://i.ibb.co/L1d8rfL/swin-1.jpg\" alt=\"drawing\" width=\"350\" style=\"padding: 20px;\"/>\n    <p style='text-align: center;'>\n        <span style=\"color: #353535; font-size:0.9em; font-family: Verdana; font-weight: 300; letter-spacing: 0px;\">(a) Building hierarchical feature maps by merging image patches in Swin transformers; (b) global computation of self-attention in ViT.</span>\n    </p>\n</div>\n<br>\n\nSwin transformers are a class of visual transformer-based neural network architectures aimed at improving the adaptation of transformers for vision tasks similar to ViT/DeiT. They have achieved state-of-the-art results across various tasks such as image classification, instance segmentation, and semantic segmentation. They take a hierarchical approach in its architecture in building feature maps by merging patches (keeping the number of patches in each layer a constant with respect to the image size), when moving from one layer to the other to achieve <span style=\"color: #DC143C;\">scale-invariance</span>. Self-attention is computed only within each local window, thereby <span style=\"color: #DC143C;\">reducing computations to linear complexity</span> compared to the quadratic complexity of ViTs, where self-attention is computed globally. Local self-attention lacks connections across windows, limiting modelling power, and this is solved by cyclic shifting when the image is partitioned for creating patches to essentially enable <span style=\"color: #DC143C;\">cross-window connections</span>. This partitioning configuration is alternated to form consecutive non-shifted and shifted blocks, enhancing the overall modelling power.\n\n<br>\n<div style=\"text-align:center\">\n    <img src=\"https://i.ibb.co/Pxz9fWG/swin-3.jpg\" alt=\"drawing\" width=\"650\" style=\"padding: 20px;\"/>\n    <p style='text-align: center;'>\n        <span style=\"color: #353535; font-size:0.9em; font-family: Verdana; font-weight: 300; letter-spacing: 0px;\">(a) The architecture of a Swin Transformer (Swin-T); (b) two successive Swin Transformer Blocks. W-MSA and SW-MSA are multi-head self-attention modules with regular and shifted windowing configurations, respectively.</span>\n    </p>\n</div>\n<br>\n\nWe will make use of an image-restoration model proposed in the paper [SwinIR: Image Restoration Using Swin Transformer](https://arxiv.org/pdf/2108.10257.pdf), which is built upon swin transformer blocks. The generated image after $N$ CLIP-conditioned diffusion denoising steps is fed as the input to this model. The architecture of SwinIR consists of modules for shallow feature extraction, deep feature extraction, and high-quality (HQ) image reconstruction. Shallow feature extraction module extracts the shallow features which have the low-frequency information. By means of a convolution layer and these are directly transmitted to the final reconstruction module. Deep feature extraction module consists of several Residual Swin Transformer blocks (RSTB). Each RSTB has several swin transformer layers for capturing local attention and cross-window interactions. The authors also use another convolution layer at the end of the block for feature enhancement with a residual connection, to provide a shortcut for feature aggregation. Both the shallow and deep features are fused at the final reconstruction module, producing the final restored or enlarged image.\n\n<br>\n<div style=\"text-align:center\">\n    <img src=\"https://i.ibb.co/RpK5xCt/swinir.jpg\" alt=\"drawing\" width=\"550\"/>\n    <p style='text-align: center;'>\n        <span style=\"color: #353535; font-size:0.9em; font-family: Verdana; font-weight: 300; letter-spacing: 0px;\">Figure 5.3: SwinIR architecture</span>\n    </p>\n</div>\n<br>\n\n<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Credits</span>\n\nDeveloped using techniques and architectures borrowed from original work by the authors below:\n\n- [Guided diffusion](https://github.com/openai/guided-diffusion) and [improved diffusion](https://github.com/openai/improved-diffusion) by [OpenAI](https://github.com/openai)\n\n- Original notebook on CLIP guidance sampling by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings) with improvements by [nerdyrodent](https://github.com/nerdyrodent/CLIP-Guided-Diffusion) and [sadnow](https://github.com/sadnow/360Diffusion) (@sadly_existent) \n\n- [SwinIR: Image Restoration Using Shifted Window Transformer](https://github.com/JingyunLiang/SwinIR)\n\nHuge thanks to all their great work! I highly recommend checking these out.\n\n<br>\n<span style=\"float:center;\"><a href=\"https://www.kaggle.com/sreevishnudamodaran\"><img style=\"padding: 5px;\" border=\"0\" alt=\"Ask Me Something\" src=\"https://img.shields.io/badge/Ask%20Me%20Something-FCC624?style=for-the-badge&logo=kaggle&logoColor=black\" width=\"175\"></a><br>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Install Dependencies</span>","metadata":{}},{"cell_type":"code","source":"!conda install -y mpi4py >> /dev/null\n\n!git clone https://github.com/sreevishnu-damodaran/clip-diffusion-art.git -q\n%cd /kaggle/working/clip-diffusion-art\n!pip install -e . -q\n!git clone https://github.com/crowsonkb/guided-diffusion -q\n!pip install -e guided-diffusion -q\n!git clone https://github.com/JingyunLiang/SwinIR.git -q\n!git clone https://github.com/openai/CLIP -q\n!pip install -e ./CLIP -q","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T06:33:08.660378Z","iopub.execute_input":"2022-03-10T06:33:08.660719Z","iopub.status.idle":"2022-03-10T06:34:52.833498Z","shell.execute_reply.started":"2022-03-10T06:33:08.660627Z","shell.execute_reply":"2022-03-10T06:34:52.832651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">‚öóÔ∏è Imports & Setup</span>","metadata":{}},{"cell_type":"code","source":"import random\nimport os\nimport numpy as np\n\nimport sys\nimport yaml\nimport glob\nfrom datetime import datetime\n\nimport matplotlib.pyplot as plt\nfrom types import SimpleNamespace\nimport wandb\n\nimport torch\nimport torchvision\nimport torchvision.transforms.functional as TF\n\nsys.path.append(\"./clip-diffusion-art\")\nsys.path.append(\"./guided-diffusion\")\nfrom clip_diffusion_art import logger\nfrom clip_diffusion_art.train import TrainLoop\nfrom clip_diffusion_art.cda_utils import (\n    args_to_dict,\n    add_dict_to_argparser,\n)\nfrom clip_diffusion_art.sample import ClipDiffusion\n\nfrom guided_diffusion.script_util import (\n    create_model_and_diffusion,\n    model_and_diffusion_defaults\n)\nfrom guided_diffusion.image_datasets import load_data\nfrom guided_diffusion.resample import create_named_schedule_sampler\nfrom guided_diffusion import dist_util","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:34:52.837085Z","iopub.execute_input":"2022-03-10T06:34:52.837334Z","iopub.status.idle":"2022-03-10T06:34:55.818063Z","shell.execute_reply.started":"2022-03-10T06:34:52.837307Z","shell.execute_reply":"2022-03-10T06:34:55.817175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def seed_all(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:34:55.820065Z","iopub.execute_input":"2022-03-10T06:34:55.820323Z","iopub.status.idle":"2022-03-10T06:34:55.826723Z","shell.execute_reply.started":"2022-03-10T06:34:55.820287Z","shell.execute_reply":"2022-03-10T06:34:55.825049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">‚öíÔ∏è Training Config & Hyperparameters</span>\n\n<br>\n\nIn this step, we will chooose the hyperparameters and other training configurations for fine-tuning with the custom dataset. We have selected resonable defaults which allows us to fine-tune a model on custom datasets with 16GB GPUs on Colab or Kaggle.\n\n### Download the pre-trained checkpoint.","metadata":{}},{"cell_type":"code","source":"!wget https://openaipublic.blob.core.windows.net/diffusion/march-2021/lsun_uncond_100M_1200K_bs128.pt -P ./pretrained_models -q\n\nresume_checkpoint = \"/kaggle/working/clip-diffusion-art/pretrained_models/lsun_uncond_100M_1200K_bs128.pt\"","metadata":{"execution":{"iopub.status.busy":"2022-03-10T06:34:55.829183Z","iopub.execute_input":"2022-03-10T06:34:55.829442Z","iopub.status.idle":"2022-03-10T06:35:19.903115Z","shell.execute_reply.started":"2022-03-10T06:34:55.829408Z","shell.execute_reply":"2022-03-10T06:35:19.902135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_cfg = model_and_diffusion_defaults()\n\ncfg = {\n    'data_dir': \"/kaggle/input/artworks-in-public-domain/artworks_in_public_domain\",\n    'attention_resolutions': \"16\",\n    'class_cond': False,\n    'diffusion_steps':1000,\n    'rescale_timesteps': True,\n    'rescale_learned_sigmas': True,\n    'image_size': 256,\n    'learn_sigma': True,\n    'noise_schedule': \"linear\",\n    'num_channels': 128,\n    'num_heads': 1,\n    'num_res_blocks': 2,\n    'use_checkpoint': False,\n    'use_fp16': True,\n    'use_scale_shift_norm': False,\n    'schedule_sampler': \"uniform\",\n    'lr': 1e-7,\n    'weight_decay': 0.0,\n    'lr_anneal_steps': 0,\n    'batch_size': 8,\n    'microbatch': 1,  # -1 disables microbatches\n    'ema_rate': \"0.9999\",  # comma-separated list of EMA values\n    'log_interval': 10,\n    'save_interval': 1000,\n    'resume_checkpoint': resume_checkpoint,\n    'use_checkpoint': True,\n    'fp16_scale_growth': 1e-3,\n    'log_dir': \"outputs\",\n    'wandb_project': \"clip_diffusion_art_train\",\n    'wandb_entity': None,\n    'wandb_name': None,\n    'seed': 47\n}\n\ntrain_cfg.update(cfg)\ntrain_cfg = SimpleNamespace(**train_cfg)\n\n# Set seed for training\nseed_all(train_cfg.seed)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:12:28.521637Z","iopub.execute_input":"2022-03-10T07:12:28.522326Z","iopub.status.idle":"2022-03-10T07:12:28.530555Z","shell.execute_reply.started":"2022-03-10T07:12:28.522277Z","shell.execute_reply":"2022-03-10T07:12:28.529737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Dataset Creation Process</span>\n\nI have downloaded artworks that are in the public domain from [WikiArt](https://www.wikiart.org/) and [rawpixel.com](https://www.rawpixel.com/) for creating the dataset used for this project. After downloading them, I resized everything to the size of 256x256. The dataset contains around 29.3k images. We will use this dataset to fine-tune our model.\n\nTo use custom datasets for training, download/scrape the necessary images, and then resize them (and preferably center crop to avoid aspect ratio change) to the input size of the diffusion model of choice.\n\nNote: Make sure all the images have 3 channels (RGB). In case of grayscale images, convert them to RGB.\n\n<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Load & Display Training Samples</span>\n\n`load_data` function creates the torch dataset and a torch dataloader-based generator for training.","metadata":{}},{"cell_type":"code","source":"data = load_data(\n    data_dir=train_cfg.data_dir,\n    batch_size=train_cfg.batch_size,\n    image_size=train_cfg.image_size,\n    class_cond=train_cfg.class_cond,\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:12:30.197186Z","iopub.execute_input":"2022-03-10T07:12:30.197966Z","iopub.status.idle":"2022-03-10T07:12:30.207067Z","shell.execute_reply.started":"2022-03-10T07:12:30.197914Z","shell.execute_reply":"2022-03-10T07:12:30.206264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_imgs, _ = next(data)\ngrid_img = torchvision.utils.make_grid(sample_imgs.clamp(-1, 1).add(1).div(2), nrow=4)\nplt.figure(figsize=(10, 5))\nplt.axis('off')\nplt.imshow(grid_img.permute(1, 2, 0));","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:12:30.92661Z","iopub.execute_input":"2022-03-10T07:12:30.926847Z","iopub.status.idle":"2022-03-10T07:12:41.883378Z","shell.execute_reply.started":"2022-03-10T07:12:30.92682Z","shell.execute_reply":"2022-03-10T07:12:41.882605Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"text-align:center\">\n    <img src=\"https://i.ibb.co/1sQcJb2/gb6B4ig.png\" alt=\"drawing\" width=\"350\"/></div>\n\n<br>\n<br>\n\nWeights & Biases helps machine learning teams build better models faster. With a few lines of code, practitioners can instantly debug, compare and reproduce their models ‚Äî architecture, hyperparameters, git commits, model weights, GPU usage, and even datasets and predictions ‚Äî and collaborate with their teammates.\n\nNow, fast track your experiments with:\n\n - **Dashboard (experiment tracking)**: Log and visualize experiments in real time = Keep data and results in one convenient place. Consider this as a repository of experiments.\n - **Artifacts (dataset + model versioning)**: Store and version datasets, models, and results = Know exactly what data a model is being trained on.\n\nIt is also <span style=\"color: #DC143C;\">free to use for academic and open source projects!</span>\n\n<!-- <span style=\"background-color: #EFFFCD;\">üìå Dataset with public domain artworks created for this project:</span><br><br>\n<span>\n&emsp;<a href=\"https://www.kaggle.com/sreevishnudamodaran/artworks-in-public-domain\">kaggle.com/sreevishnudamodaran/artworks-in-public-domain\n</a>\n</span> -->\n\n**Visit https://wandb.me/kaggle to know more about using Weights & Biases in Kaggle!**\n\n**To get to know about all the exciting features, take a look at https://wandb.ai/site**\n\n<br>\n<br>\n\n<p style='text-align: left;'><span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">‚öóÔ∏è Integrate Weights & Biases</span></p>\n\nI have already integrated Weights & Biases to perform logging of metrics and images in the repository we use.\n\nTo enable it, just pass `wandb_run` handler created below to the training loop method for experiment tracking and logging.","metadata":{}},{"cell_type":"code","source":"wandb_run = wandb.init(project=train_cfg.wandb_project,\n    entity=train_cfg.wandb_entity,\n    name=train_cfg.wandb_name)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:12:49.063191Z","iopub.execute_input":"2022-03-10T07:12:49.06349Z","iopub.status.idle":"2022-03-10T07:12:55.542567Z","shell.execute_reply.started":"2022-03-10T07:12:49.063455Z","shell.execute_reply":"2022-03-10T07:12:55.541809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Training</span>","metadata":{}},{"cell_type":"code","source":"dist_util.setup_dist()\n\nmodel, diffusion = create_model_and_diffusion(\n    **args_to_dict(train_cfg, model_and_diffusion_defaults().keys()))\nmodel.to(dist_util.dev())\n\nschedule_sampler = create_named_schedule_sampler(\n    train_cfg.schedule_sampler, diffusion)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:12:55.546867Z","iopub.execute_input":"2022-03-10T07:12:55.547491Z","iopub.status.idle":"2022-03-10T07:12:57.183229Z","shell.execute_reply.started":"2022-03-10T07:12:55.547462Z","shell.execute_reply":"2022-03-10T07:12:57.18171Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logger.configure(dir=train_cfg.log_dir,\n                    wandb_run=wandb_run)\n\ntry:\n    TrainLoop(\n    model=model,\n    diffusion=diffusion,\n    data=data,\n    batch_size=train_cfg.batch_size,\n    microbatch=train_cfg.microbatch,\n    lr=train_cfg.lr,\n    ema_rate=train_cfg.ema_rate,\n    log_interval=train_cfg.log_interval,\n    save_interval=train_cfg.save_interval,\n    resume_checkpoint=train_cfg.resume_checkpoint,\n    use_fp16=train_cfg.use_fp16,\n    fp16_scale_growth=train_cfg.fp16_scale_growth,\n    schedule_sampler=schedule_sampler,\n    weight_decay=train_cfg.weight_decay,\n    lr_anneal_steps=train_cfg.lr_anneal_steps,\n    wandb_run=wandb_run\n).run_loop()\n    \nexcept KeyboardInterrupt:\n    wandb.finish()","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-03-10T07:12:57.18801Z","iopub.execute_input":"2022-03-10T07:12:57.190288Z","iopub.status.idle":"2022-03-10T07:28:09.505728Z","shell.execute_reply.started":"2022-03-10T07:12:57.190246Z","shell.execute_reply":"2022-03-10T07:28:09.50505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training can also be done in script mode\n\n\n#### Set Hyperparameters\n\n```\nMODEL_FLAGS=\"--image_size 256 --num_channels 128 --num_res_blocks 2 --num_heads 1 --attention_resolutions 16\"\nDIFFUSION_FLAGS=\"--diffusion_steps 1000 --noise_schedule linear --learn_sigma True --rescale_learned_sigmas True --rescale_timesteps True --use_scale_shift_norm False\"\nTRAIN_FLAGS=\"--lr 5e-6 --save_interval 500 --batch_size 16 --use_fp16 True --wandb_project diffusion-art-train --use_checkpoint True --resume_checkpoint pretrained_models/lsun_uncond_100M_1200K_bs128.pt\"\n```\n\n#### Run the Traning Job:\n\n```\npython clip_diffusion_art/train.py --data_dir path/to/images $MODEL_FLAGS $DIFFUSION_FLAGS $TRAIN_FLAGS\n```","metadata":{}},{"cell_type":"markdown","source":"<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Sampling</span>\n\nLet's download and use a checkpoint that was trained earlier for 5000 iterations on the same artworks-in-public-domain dataset, to generate samples. Do note that the we are using a fine-tuned checkpoint trained on a small number of iterations with single 16GB GPUs for demonstration purposes. Other practical applications may need more hyper-parameter tuning, longer training, and larger pre-trained models.","metadata":{}},{"cell_type":"code","source":"!wget https://api.wandb.ai/files/sreevishnu-damodaran/clip_diffusion_art/29bag3br/256x256_clip_diffusion_art.pt -q","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:28:30.107158Z","iopub.execute_input":"2022-03-10T07:28:30.107744Z","iopub.status.idle":"2022-03-10T07:28:34.201524Z","shell.execute_reply.started":"2022-03-10T07:28:30.107705Z","shell.execute_reply":"2022-03-10T07:28:34.200559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #000508; font-size:1.3em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Sampling Config </span>\n\n#### Options:\n`--images` - image prompts (default=None)<br>\n`--checkpoint` - diffusion model checkpoint to use for sampling<br>\n`--model_config` - diffusion model config yaml<br>\n`--wandb_project` - enable wandb logging and use this project name<br>\n`--wandb_name` - optinal run name to use for wandb logging<br>\n`--wandb_entity` - optinal entity to use for wandb logging<br>\n`--num_samples` - - number of samples to generate (default=1)<br>\n`--batch_size` - default=1batch size for the diffusion model<br>\n`--sampling` - timestep respacing sampling methods to use (default=\"ddim50\", choices=[25, 50, 100, 150, 250, 500, 1000, ddim25, ddim50, ddim100, ddim150, ddim250, ddim500, ddim1000])<br>\n`--diffusion_steps` - number of diffusion timesteps (default=1000)<br>\n`--skip_timesteps` - diffusion timesteps to skip (default=5)<br>\n`--clip_denoised` - enable to filter out noise from generation (default=False)<br>\n`--randomize_class_disable` - disables changing imagenet class randomly in each iteration (default=False)<br>\n`--eta` - the amount of noise to add during sampling (default=0)<br>\n`--clip_model` - CLIP pre-trained model to use (default=\"ViT-B/16\",\nchoices=[\"RN50\",\"RN101\",\"RN50x4\",\"RN50x16\",\"RN50x64\",\"ViT-B/32\",\"ViT-B/16\",\"ViT-L/14\"])<br>\n`--skip_augs` - enable to skip torchvision augmentations (default=False)<br>\n`--cutn` - the number of random crops to use (default=16)<br>\n`--cutn_batches` - number of crops to take from the image (default=4)<br>\n`--init_image` - init image to use while sampling (default=None)<br>\n`--loss_fn` - loss fn to use for CLIP guidance (default=\"spherical\", choices=[\"spherical\" \"cos_spherical\"])<br>\n`--clip_guidance_scale` - CLIP guidance scale (default=5000)<br>\n`--tv_scale` - controls smoothing in samples (default=100)<br>\n`--range_scale` - controls the range of RGB values in samples (default=150)<br>\n`--saturation_scale` - controls the saturation in samples (default=0)<br>\n`--init_scale` - controls the adherence to the init image (default=1000)<br>\n`--scale_multiplier` - scales clip_guidance_scale tv_scale and range_scale (default=50)<br>\n`--disable_grad_clamp` - disable gradient clamping (default=False)<br>\n`--sr_model_path` - SwinIR super-resolution model checkpoint (default=None)<br>\n`--large_sr` - enable to use large SwinIR super-resolution model (default=False)<br>\n`--output_dir` - output images directory (default=\"output_dir\")<br>\n`--seed` - the random seed (default=47)<br>\n`--device` - the device to use <br>\n<br>\n","metadata":{}},{"cell_type":"code","source":"cfg_dict = {\n    \"seed\": 84,\n    \"wandb_project\": \"clip_diffusion_art\",\n    \"wandb_name\": \"job7\",\n    \"model_config\": \"clip_diffusion_art/configs/256x256_clip_diffusion_art.yaml\",\n    \"checkpoint\": \"/kaggle/working/clip-diffusion-art/256x256_clip_diffusion_art.pt\",\n    \"batch_size\": 1,\n    \"skip_timesteps\": 5,\n    \"sampling\": \"ddim50\",\n    \"diffusion_steps\": 1000,\n    \"clip_guidance_scale\": 5000,\n    \"cutn\": 60,\n    \"cutn_batches\": 4,\n    \"scale_multiplier\": 1,\n    \"tv_scale\":75,\n    \"range_scale\": 200,\n    \"loss_fn\":\"spherical\",\n    \"clip_model\": \"ViT-B/16\",\n    \"large_sr\": True,\n}\n\ncfg_dict[\"output_dir\"] = f\"/kaggle/working/{cfg_dict['wandb_name']}\"\n\ncfg = SimpleNamespace(**cfg_dict)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:28:34.203988Z","iopub.execute_input":"2022-03-10T07:28:34.204258Z","iopub.status.idle":"2022-03-10T07:28:34.211044Z","shell.execute_reply.started":"2022-03-10T07:28:34.204222Z","shell.execute_reply":"2022-03-10T07:28:34.210202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seed_all(cfg.seed)\n\nconfig_file = open(cfg.model_config)\nmodel_config = yaml.load(config_file,\n                         Loader=yaml.FullLoader)[\"model_config\"]\nprint(\"model_config\", model_config)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:28:34.212839Z","iopub.execute_input":"2022-03-10T07:28:34.213411Z","iopub.status.idle":"2022-03-10T07:28:34.230013Z","shell.execute_reply.started":"2022-03-10T07:28:34.213368Z","shell.execute_reply":"2022-03-10T07:28:34.229086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clip_diffusion = ClipDiffusion(cfg.checkpoint,\n    model_config=model_config,\n    sampling=cfg.sampling,\n    diffusion_steps=cfg.diffusion_steps,\n    clip_model=cfg.clip_model,\n    device=device\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:28:34.232246Z","iopub.execute_input":"2022-03-10T07:28:34.232719Z","iopub.status.idle":"2022-03-10T07:28:55.68731Z","shell.execute_reply.started":"2022-03-10T07:28:34.232684Z","shell.execute_reply":"2022-03-10T07:28:55.68653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Generate Samples</span>","metadata":{}},{"cell_type":"code","source":"os.makedirs(os.path.join(cfg.output_dir, 'wandb'), exist_ok=True)\n\nwandb_run = wandb.init(project=cfg.wandb_project,\n                        dir=cfg.output_dir,\n                        name=cfg.wandb_name)","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:28:55.688446Z","iopub.execute_input":"2022-03-10T07:28:55.688887Z","iopub.status.idle":"2022-03-10T07:29:03.305746Z","shell.execute_reply.started":"2022-03-10T07:28:55.688847Z","shell.execute_reply":"2022-03-10T07:29:03.304938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Give your prompt of choice to generate artworks\n\n**Some examples to try:**\n\n\"beautiful matte painting of dystopian city, Behance HD\"<br>\n\"vibrant watercolor painting of a flower, artstation HQ\"<br>\n\"a photo realistic apple in HD\"<br>\n\"beach with glowing neon lights, trending on artstation\"<br>\n\"beautiful abstract painting of the horizon in ultrafine detail, HD\"<br>\n\"vibrant digital illustration of a waterfall in the woods, HD\"<br>\n\"beautiful matte painting of ship at sea, Behance HD\"<br>\n\"hyperrealism oil painting of beautiful skies, HD\"","metadata":{}},{"cell_type":"code","source":"prompts =  [\"vibrant matte painting of a house in an enchanted forest, artstation HQ\"]\nnum_samples = 4\n\nout_generator = clip_diffusion.sample(\n                    prompts,\n#                     args.images,\n                    num_samples=num_samples,\n                    batch_size=cfg.batch_size,\n                    skip_timesteps=cfg.skip_timesteps,\n#                     clip_denoised=cfg.clip_denoised,\n#                     randomize_class=cfg.randomize_class,\n#                     eta=cfg.eta,\n#                     skip_augs=cfg.skip_augs,\n                    cutn=cfg.cutn,\n                    cutn_batches=cfg.cutn_batches,\n#                     init_image=cf.init_image,\n                    loss_fn=cfg.loss_fn,\n                    clip_guidance_scale=cfg.clip_guidance_scale,\n                    tv_scale=cfg.tv_scale,\n                    range_scale=cfg.range_scale,\n#                     saturation_scale=cfg.saturation_scale,\n#                     init_scale=cfg.init_scale,\n                    scale_multiplier=cfg.scale_multiplier,\n                    output_dir=cfg.output_dir,\n                    wandb_run=wandb_run\n                )","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:29:03.310268Z","iopub.execute_input":"2022-03-10T07:29:03.312298Z","iopub.status.idle":"2022-03-10T07:29:03.3229Z","shell.execute_reply.started":"2022-03-10T07:29:03.312257Z","shell.execute_reply":"2022-03-10T07:29:03.32216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.makedirs(cfg.output_dir, exist_ok=True)\n\nfor i, out_image in enumerate(out_generator):\n    disp_image = TF.to_pil_image(out_image.squeeze(0))\n    out_image = clip_diffusion.upscale(out_image,\n                                        large_sr=cfg.large_sr)\n    out_image = TF.to_pil_image(out_image.squeeze(0))\n    \n    fig, axs = plt.subplots(1, 2, figsize=(15, 18))\n    [axi.set_axis_off() for axi in axs.ravel()]\n    axs[0].imshow(disp_image)\n    axs[0].set_title(\"Before Super-resolution\", fontsize=20)\n    axs[1].imshow(out_image)\n    axs[1].set_title(\"After Super-resolution\", fontsize=20)\n    plt.show()\n    \n    current_time = datetime.now().strftime('%y%m%d-%H%M%S_%f')\n    filename = f'image{i}_{current_time}.png'\n    out_image.save(os.path.join(cfg.output_dir, filename))\n    \n    if wandb_run is not None:\n        wandb.log({os.path.splitext(filename)[0]: wandb.Image(os.path.join(cfg.output_dir, filename))})","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:29:03.326911Z","iopub.execute_input":"2022-03-10T07:29:03.329851Z","iopub.status.idle":"2022-03-10T07:41:23.685075Z","shell.execute_reply.started":"2022-03-10T07:29:03.329801Z","shell.execute_reply":"2022-03-10T07:41:23.679931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if wandb_run is not None:\n    for k in range(cfg.batch_size):\n        for i in range(num_samples):\n            img_files = glob.glob(os.path.join(cfg.output_dir,\n                                        f\"sample{i}_output{k}_steps\", '*'))\n            wandb.log(\n                {f\"sample{i}_output{k}\": [wandb.Image(img_path) for img_path\n                in sorted(img_files,\n                          key=lambda x: int(os.path.splitext(x)[0].split(\"_\")[-1].lstrip(\"step\")))]}\n            )\n                      \nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2022-03-10T07:41:23.687529Z","iopub.execute_input":"2022-03-10T07:41:23.687957Z","iopub.status.idle":"2022-03-10T07:41:34.113451Z","shell.execute_reply.started":"2022-03-10T07:41:23.687919Z","shell.execute_reply":"2022-03-10T07:41:34.112703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<span style=\"color: #674177; font-size:1.4em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Generated Results</span>\n\n<span style=\"font-size:1.1em;\">\n    <b>üìå View more generated artworks<a href=\"https://wandb.ai/sreevishnu-damodaran/clip_diffusion_art/reports/Results-CLIP-Guided-Diffusion-SwinIR--VmlldzoxNjUxNTMz\"> here\n<br><br>\n</a>\n</b>\n</span>\n\n<br>\n\n<div style=\"text-align:center\">\n     <span style=\"color: #353535; font-size:1.1em; font-family: Verdana; font-weight: 300; letter-spacing: 0px;\">\"beautiful matte painting of a dystopian city, Behance HD\"</span>\n     <p style='text-align: center;'>\n         <img src=\"https://i.ibb.co/BTWfbf4/23-0.gif\" alt=\"drawing\" width=\"300\" style=\"padding: 20px;\"/>\n    </p>\n</div>\n\n<div style=\"text-align:center\">\n     <span style=\"color: #353535; font-size:1.1em; font-family: Verdana; font-weight: 300; letter-spacing: 0px;\">\"vibrant watercolor painting of a flower, artstation HQ\"</span>\n     <p style='text-align: center;'>\n         <img src=\"https://i.ibb.co/8dBTzpX/job18-2.gif\" alt=\"drawing\" width=\"300\" style=\"padding: 20px;\"/>\n    </p>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<span style=\"color: #674177; font-size:1.6em; font-family: Segoe UI; font-weight: 600; letter-spacing: 0px;\">Super-resolution Results</span>\n\n<div style=\"text-align:center\">\n    <img src=\"https://i.ibb.co/Gss0y38/sr-zoom-optimized.gif\" alt=\"drawing\" width=\"650\" style=\"padding: 20px;\"/>\n</div>","metadata":{}},{"cell_type":"markdown","source":"<p style='text-align: center;'><span style=\"color: #006ea4; font-family: Tahoma; font-size: 1.8em; font-weight: 300;\">Thanks for reading!</span></p>","metadata":{}}]}